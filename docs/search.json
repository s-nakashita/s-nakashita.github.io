[
  {
    "objectID": "L63_PLS.html",
    "href": "L63_PLS.html",
    "title": "Lorenz-63",
    "section": "",
    "text": "Code\nimport numpy as np\nCode\nclass L63():\n    def __init__(self):\n        self.p = 10.0 # prandtl number\n        self.r = 32.0 # rayleigh number\n        self.b = 8.0/3.0 # aspect ratio\n        self.dt = 0.01\n\n    def __call__(self,w):\n        x, y, z = w \n        dw = np.zeros_like(w)\n        dw[0] = -self.p * x + self.p * y\n        dw[1] = -x * z      + self.r * x - y\n        dw[2] = x * y       - self.b * z\n        return w + dw * self.dt\n    \n    def tlm(self,wb,wt):\n        xb, yb, zb = wb\n        xt, yt, zt = wt \n        dw = np.zeros_like(wt)\n        dw[0] = -self.p * xt     + self.p * yt\n        dw[1] = (self.r - zb)*xt - yt          - xb*zt\n        dw[2] = yb*xt            + xb*yt       - self.b * zt\n        return wt + dw * self.dt\n    \n    def adj(self,wb,wt):\n        xb, yb, zb = wb\n        xt, yt, zt = wt \n        dw = np.zeros_like(wt)\n        dw[0] = -self.p * xt + (self.r - zb)*yt + yb*zt\n        dw[1] =  self.p * xt - yt               + xb*zt\n        dw[2] =              - xb*yt            - self.b * zt\n        return wt + dw * self.dt\nCode\nimport matplotlib.pyplot as plt \n\nmodel = L63()\nnstep = 500\n\nw1 = np.zeros((nstep+1,3))\nw2 = np.zeros((nstep+1,3))\nw1[0,:] = 1.0,3.0,5.0\nw2[0,:] = 1.1,3.3,5.5\nfor i in range(nstep):\n    w1[i+1,] = model(w1[i])\n    w2[i+1,] = model(w2[i])\n\nfig = plt.figure(figsize=[8,8])\nax = fig.add_subplot(projection=\"3d\")\nax.plot(*w1.transpose())\nax.plot(*w2.transpose())\nplt.show()"
  },
  {
    "objectID": "L63_PLS.html#tlm-and-adj-check",
    "href": "L63_PLS.html#tlm-and-adj-check",
    "title": "Lorenz-63",
    "section": "TLM and ADJ check",
    "text": "TLM and ADJ check\n\n\nCode\nmodel = L63()\n# TLM check\nw0 = np.random.randn(3)\ndw0 = np.random.randn(3)*0.1\nalp = 1.0e-5\nwp = model(w0+alp*dw0)\nwb = model(w0)\ndw = model.tlm(w0,dw0)\nratio = np.sqrt(np.dot((wp-wb),(wp-wb)))/alp/np.sqrt(np.dot(dw,dw))\nprint(f\"|M(x+a*dx)-M(x)|/|a*TLM*dx|={ratio}\")\n# ADJ check\nMtMw = model.adj(w0,dw)\nprint(f\"(Mx)^T(Mx)-x^TM^TMx={np.dot(dw,dw)-np.dot(dw0,MtMw)}\")\n\n\n|M(x+a*dx)-M(x)|/|a*TLM*dx|=1.0000000015309585\n(Mx)^T(Mx)-x^TM^TMx=0.0"
  },
  {
    "objectID": "L63_PLS.html#singular-vector",
    "href": "L63_PLS.html#singular-vector",
    "title": "Lorenz-63",
    "section": "Singular vector",
    "text": "Singular vector\n\n\nCode\n## base field\nwb = np.array([1.0,3.0,5.0])\nnstep = 100\nfor i in range(nstep):\n    wb = model(wb)\n\n\n\nLanczos method\n\n\nCode\n# initialize\nalpha=1.0e-5\ndw = np.random.randn(3)\ndw = dw * alpha / np.sqrt(np.dot(dw,dw))\nniter=0\nwhile(True):\n    dwp = dw.copy()\n    # forward integration\n    dw = model.tlm(wb,dw)\n    # backward integration\n    dw = model.adj(wb,dw)\n    # rescaling\n    dw = dw * alpha / np.sqrt(np.dot(dw,dw))\n    # convergence evaluation\n    diff = np.sqrt(np.dot((dw-dwp),(dw-dwp)))\n    niter+=1\n    print(f\"iter:{niter}, diff={diff:.3e}\")\n    if diff/alpha&lt;1.0e-6: break\nprint(dw/alpha)\n\n\niter:1, diff=1.707e-06\niter:2, diff=2.408e-06\niter:3, diff=2.730e-06\niter:4, diff=2.365e-06\niter:5, diff=1.666e-06\niter:6, diff=1.066e-06\niter:7, diff=6.739e-07\niter:8, diff=4.423e-07\niter:9, diff=3.085e-07\niter:10, diff=2.287e-07\niter:11, diff=1.774e-07\niter:12, diff=1.414e-07\niter:13, diff=1.144e-07\niter:14, diff=9.323e-08\niter:15, diff=7.623e-08\niter:16, diff=6.243e-08\niter:17, diff=5.116e-08\niter:18, diff=4.193e-08\niter:19, diff=3.438e-08\niter:20, diff=2.818e-08\niter:21, diff=2.311e-08\niter:22, diff=1.894e-08\niter:23, diff=1.553e-08\niter:24, diff=1.273e-08\niter:25, diff=1.044e-08\niter:26, diff=8.559e-09\niter:27, diff=7.017e-09\niter:28, diff=5.753e-09\niter:29, diff=4.716e-09\niter:30, diff=3.867e-09\niter:31, diff=3.170e-09\niter:32, diff=2.599e-09\niter:33, diff=2.131e-09\niter:34, diff=1.747e-09\niter:35, diff=1.432e-09\niter:36, diff=1.174e-09\niter:37, diff=9.625e-10\niter:38, diff=7.891e-10\niter:39, diff=6.469e-10\niter:40, diff=5.304e-10\niter:41, diff=4.348e-10\niter:42, diff=3.565e-10\niter:43, diff=2.923e-10\niter:44, diff=2.396e-10\niter:45, diff=1.964e-10\niter:46, diff=1.610e-10\niter:47, diff=1.320e-10\niter:48, diff=1.082e-10\niter:49, diff=8.874e-11\niter:50, diff=7.275e-11\niter:51, diff=5.964e-11\niter:52, diff=4.890e-11\niter:53, diff=4.009e-11\niter:54, diff=3.287e-11\niter:55, diff=2.694e-11\niter:56, diff=2.209e-11\niter:57, diff=1.811e-11\niter:58, diff=1.485e-11\niter:59, diff=1.217e-11\niter:60, diff=9.979e-12\n[ 0.59253215  0.76624248 -0.24855203]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ensemble sensitivity analysis with Python",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "pyesa/about.html",
    "href": "pyesa/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "pyesa/index.html",
    "href": "pyesa/index.html",
    "title": "Ensemble sensitivity analysis with Python",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "EnASA_L96.html",
    "href": "EnASA_L96.html",
    "title": "Comparison of adjoint sensitivity analysis and ensemble adjoint sensitivity analysis using Lorenz-96 model",
    "section": "",
    "text": "Code\nimport os\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nfrom numpy.random import default_rng"
  },
  {
    "objectID": "EnASA_L96.html#model-lorenz-96",
    "href": "EnASA_L96.html#model-lorenz-96",
    "title": "Comparison of adjoint sensitivity analysis and ensemble adjoint sensitivity analysis using Lorenz-96 model",
    "section": "Model: Lorenz 96",
    "text": "Model: Lorenz 96\n\\[\n\\frac{\\mathrm{d}X_n}{\\mathrm{d}t} = -X_{n-2}X_{n-1}+X_{n-1}X_{n+1}-X_n+F\n\\tag{1}\\]\n\n\nCode\nsys.path.append(os.environ['HOME']+'/Development/pydpac/model')\nfrom lorenz import L96\n\nnx=40\ndt=0.05/6.0 # = 1 hour\nF=8.0\n\nmodel = L96(nx,dt,F)\n\n\nnx=40 F=8.0 dt=0.008333333333333333\n\n\n\nbasic state\n\n\nCode\n# initialize random seed\nrng = default_rng(509)\n# spinup\nx = rng.normal(0.0,size=nx,scale=1.0)\nnstep = 500\nfor i in range(nstep):\n    x = model(x)\n\nvtmax = 72 # hours\nvt = 24 # hours\nit = 21 # target point\nt = [0]\nxb = [x]\nfor i in range(vtmax):\n    x = model(x)\n    t.append(i)\n    xb.append(x)\nplt.plot(xb[0],label='initial')\nplt.plot(xb[vt],label='valid')\nplt.plot([it],xb[vt][it],marker='o',c='r')\nplt.grid()\nplt.legend()\nplt.show()\n\nmp = plt.pcolormesh(np.arange(nx),t,np.array(xb),\\\n    shading='auto',norm=Normalize(-10,10),cmap='coolwarm')\nif vt &lt; vtmax:\n    plt.hlines([vt],0,nx-1,colors='k',ls='dashed')\nplt.colorbar(mp)\nplt.show()"
  },
  {
    "objectID": "EnASA_L96.html#minimum-norm-solution",
    "href": "EnASA_L96.html#minimum-norm-solution",
    "title": "Comparison of adjoint sensitivity analysis and ensemble adjoint sensitivity analysis using Lorenz-96 model",
    "section": "minimum norm solution",
    "text": "minimum norm solution\nEnomoto et al. (2015); Hacker and Lei (2015)\n\\[\n\\left(\\frac{\\partial J_\\mathrm{e}}{\\partial \\mathbf{x}_0}\\right)_\\mathrm{minnorm}=\\mathbf{X}_0(\\mathbf{X}_0^\\mathrm{T}\\mathbf{X}_0)^\\dagger\\mathbf{J}_\\mathrm{e}\n\\tag{6}\\]\n\n\nCode\ndef enasa_minnorm(Jes,Jem,sJe,X0s,X0m,sX0):\n    dJedx0_s = np.dot(np.dot(X0s,np.linalg.pinv(np.dot(X0s.T,X0s))),Jes)\n    dJedx0 = dJedx0_s * sJe / sX0\n    err = Jem - np.dot(X0m,dJedx0)\n    return dJedx0, err\ndJedx0, err=enasa_minnorm(Jes,Jem,sJe,X0s,X0m,sX0)\ndJedx0_dict['minnorm'] = dJedx0\nerr_dict['minnorm'] = err\ndxeopt=calc_dxopt(dJedx0_dict['minnorm'],vt)\ndxopt_dict['minnorm'] = dxeopt\ncheck_djdx(dJedx0_dict['minnorm'],vt,label='dJedx0')"
  },
  {
    "objectID": "EnASA_L96.html#minimum-variance-solution",
    "href": "EnASA_L96.html#minimum-variance-solution",
    "title": "Comparison of adjoint sensitivity analysis and ensemble adjoint sensitivity analysis using Lorenz-96 model",
    "section": "minimum variance solution",
    "text": "minimum variance solution\n\\[\n\\left(\\frac{\\partial J_\\mathrm{e}}{\\partial \\mathbf{x}_0}\\right)_\\mathrm{minvar}=(\\mathbf{X}_0\\mathbf{X}_0^\\mathrm{T})^{-1}\\mathbf{X}_0\\mathbf{J}_\\mathrm{e},\n\\tag{7}\\] which cannot be determined if \\(\\mathbf{X}_0\\mathbf{X}_0^\\mathrm{T}\\) is singular (which is true for most cases).\n\n\nCode\ndef enasa_minvar(Jes,Jem,sJe,X0s,X0m,sX0):\n    dJedx0_s = np.dot(np.dot(np.linalg.inv(np.dot(X0s,X0s.T)),X0s),Jes)\n    dJedx0 = dJedx0_s * sJe / sX0\n    err = Jem - np.dot(X0m,dJedx0)\n    return dJedx0, err\n\n\n\ndiagonal approximation (Ancell and Hakim 2007) \\[\n\\left(\\frac{\\partial J_\\mathrm{e}}{\\partial \\mathbf{x}_0}\\right)_\\mathrm{diag}=(\\mathrm{diag}[\\mathbf{X}_0\\mathbf{X}_0^\\mathrm{T}])^{-1}\\mathbf{X}_0\\mathbf{J}_\\mathrm{e}\n\\tag{8}\\]\n\n\n\nCode\ndef enasa_diag(Jes,Jem,sJe,X0s,X0m,sX0):\n    dJedx0_s = np.dot(np.dot(np.eye(X0s.shape[0])/np.diag(np.dot(X0s,X0s.T)),X0s),Jes)\n    dJedx0 = dJedx0_s * sJe / sX0\n    err = Jem - np.dot(X0m,dJedx0)\n    return dJedx0, err\ndJedx0, err=enasa_diag(Jes,Jem,sJe,X0s,X0m,sX0)\ndJedx0_dict['diag'] = dJedx0\nerr_dict['diag'] = err\ndxeopt=calc_dxopt(dJedx0_dict['diag'],vt)\ndxopt_dict['diag'] = dxeopt\ncheck_djdx(dJedx0_dict['diag'],vt,label='dJedx0')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npseudoinverse \\[\n\\left(\\frac{\\partial J_\\mathrm{e}}{\\partial \\mathbf{x}_0}\\right)_\\mathrm{psd}=(\\mathbf{X}_0\\mathbf{X}_0^\\mathrm{T})^\\dagger\\mathbf{X}_0\\mathbf{J}_\\mathrm{e}\n\\tag{9}\\] This is equivalent to the minimum norm solution (Eq. 6).\n\n\n\nprincipal component regression (PCR) \\[\n\\frac{1}{K-1}\\mathbf{X}_0\\mathbf{X}_0^\\mathrm{T}=\\mathbf{P}_R\\boldsymbol{\\Lambda}\\mathbf{P}_R^\\mathrm{T}\n\\] \\[\n\\mathbf{T}_R = \\mathbf{P}_R^\\mathrm{T}\\mathbf{X}_0\n\\] \\[\n\\left(\\frac{\\partial J_\\mathrm{e}}{\\partial \\mathbf{x}_0}\\right)_\\mathrm{pcr}=\\mathbf{P}_R(\\mathbf{T}_R\\mathbf{T}_R^\\mathrm{T})^{-1}\\mathbf{T}_R\\mathbf{J}_e\n\\tag{10}\\]\n\n\n\nCode\ndef enasa_pcr(Jes,Jem,sJe,X0s,X0m,sX0):\n    from sklearn.decomposition import PCA\n    from sklearn.linear_model import LinearRegression\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.pipeline import make_pipeline\n\n    pcr = make_pipeline(PCA(n_components=None), LinearRegression())\n    pcr.fit(X0s.transpose(),Jes)\n    reg = pcr.named_steps[\"linearregression\"]\n    pca = pcr.named_steps[\"pca\"]\n    dJedx0_s = pca.inverse_transform(reg.coef_[None,:])[0,]\n    #print(dJedx0_s.shape)\n    #err_s = pca.inverse_transform(reg.intercept_[None,:])[0,]\n    dJedx0 = dJedx0_s * sJe / sX0\n    err = Jem - np.dot(X0m,dJedx0) #+ err_s\n    return dJedx0, err\ndJedx0, err=enasa_pcr(Jes,Jem,sJe,X0s,X0m,sX0)\ndJedx0_dict['pcr'] = dJedx0\nerr_dict['pcr'] = err\ndxeopt=calc_dxopt(dJedx0_dict['pcr'],vt)\ndxopt_dict['pcr'] = dxeopt\ncheck_djdx(dJedx0_dict['pcr'],vt,label='dJedx0')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nridge regression \\[\n\\left(\\frac{\\partial J_\\mathrm{e}}{\\partial \\mathbf{x}_0}\\right)_\\mathrm{ridge}=(\\mathbf{X}_0\\mathbf{X}_0^\\mathrm{T}+\\mu\\mathbf{I})^{-1}\\mathbf{X}_0\\mathbf{J}_\\mathrm{e},\n\\tag{11}\\] where \\(\\mu\\) is a hyper parameter.\n\n\n\nCode\ndef enasa_ridge(Jes,Jem,sJe,X0s,X0m,sX0,mu=0.01):\n    dJedx0_s = np.dot(np.dot(np.linalg.inv(np.dot(X0s,X0s.T)+mu*np.eye(X0s.shape[0])),X0s),Jes)\n    dJedx0 = dJedx0_s * sJe / sX0\n    err = Jem - np.dot(X0m,dJedx0)\n    return dJedx0, err\ndJedx0, err = enasa_ridge(Jes,Jem,sJe,X0s,X0m,sX0)\ndJedx0_dict['ridge'] = dJedx0\nerr_dict['ridge'] = err\ndxeopt=calc_dxopt(dJedx0_dict['ridge'],vt)\ndxopt_dict['ridge'] = dxeopt\ncheck_djdx(dJedx0_dict['ridge'],vt,label='dJedx0')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npartial least square (PLS) regression\nThe target vector is also projected onto a latent space spanned by the principal components. \\[\n\\left(\\frac{\\partial J_\\mathrm{e}}{\\partial \\mathbf{x}_0}\\right)_\\mathrm{pls}=\\mathbf{W}(\\mathbf{P}^\\mathrm{T}\\mathbf{W})^{-1}\\mathbf{d}\n\\tag{12}\\] where \\(\\mathbf{W}=[\\mathbf{w}_1,\\cdots,\\mathbf{w}_R]\\), \\(\\mathbf{P}=[\\mathbf{p}_1,\\cdots,\\mathbf{p}_R]\\), and \\(\\mathbf{d}=[d_1,\\cdots,d_R]^\\mathrm{T}\\) are determined iteratively, \\[\n\\mathbf{w}_r=\\frac{\\mathbf{X}_0^{(r)}\\mathbf{J}^{(r)}_\\mathrm{e}}{\\|\\mathbf{X}_0^{(r)}\\mathbf{J}^{(r)}_\\mathrm{e}\\|}\n, \\quad \\mathbf{t}_r=(\\mathbf{X}_0^{(r)})^\\mathrm{T}\\mathbf{w}_r\n, \\quad \\mathbf{p}_r=\\frac{\\mathbf{X}_0^{(r)}\\mathbf{t}_r}{\\|\\mathbf{t}_r\\|}\n, \\quad d_r=\\frac{\\mathbf{t}_r^\\mathrm{T}\\mathbf{J}^\\mathrm{(r)}_\\mathrm{e}}{\\|\\mathbf{t}_r\\|}\n\\]\n\\(\\mathbf{X}_0^{(r)}, \\mathbf{J}_\\mathrm{e}^{(r)}\\) are obtained from deflation.\n\n\n\nCode\ndef enasa_pls(Jes,Jem,sJe,X0s,X0m,sX0):\n    from sklearn.cross_decomposition import PLSRegression\n    pls = PLSRegression(n_components=nx)\n    pls.fit(X0s.transpose(),Jes)\n    dJedx0_s = pls.coef_[0,:]\n    #err_s = pls.intercept_[0]\n    dJedx0 = dJedx0_s * sJe / sX0\n    err = Jem - np.dot(X0m,dJedx0) #+ err_s\n    return dJedx0, err\ndJedx0, err = enasa_pls(Jes,Jem,sJe,X0s,X0m,sX0)\ndJedx0_dict['pls'] = dJedx0\nerr_dict['pls'] = err\ndxeopt=calc_dxopt(dJedx0_dict['pls'],vt)\ndxopt_dict['pls'] = dxeopt\ncheck_djdx(dJedx0_dict['pls'],vt,label='dJedx0')\n\n\n/Users/nakashita/Library/Python/3.11/lib/python/site-packages/sklearn/cross_decomposition/_pls.py:305: UserWarning: Y residual is constant at iteration 39\n  warnings.warn(f\"Y residual is constant at iteration {k}\")"
  },
  {
    "objectID": "EnASA_L96.html#comparison-of-enasa-methods",
    "href": "EnASA_L96.html#comparison-of-enasa-methods",
    "title": "Comparison of adjoint sensitivity analysis and ensemble adjoint sensitivity analysis using Lorenz-96 model",
    "section": "Comparison of EnASA methods",
    "text": "Comparison of EnASA methods\n\n\nCode\nmarkers=['o','v','s','P','X','p']\nmarker_style=dict(markerfacecolor='none')\nfig, ax = plt.subplots()\nax.plot(dJdx0,label='ASA')\nfor i,key in enumerate(dJedx0_dict.keys()):\n    ax.plot(dJedx0_dict[key],ls='dashed',marker=markers[i],label=f'EnASA,{key}',**marker_style)\nax.legend()\nax.grid()\nax.set_title('dJ/dx0')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots()\nax.plot(dxopt,label='ASA')\nfor i,key in enumerate(dxopt_dict.keys()):\n    ax.plot(dxopt_dict[key],ls='dashed',marker=markers[i],label=f'EnASA,{key}',**marker_style)\nax.legend()\nax.grid()\nax.set_title('dxopt')\nplt.show()\n\n\n\n\n\n\n\n\n\n\\[\n\\hat{\\mathbf{J}_\\mathrm{e}}=\\mathbf{X}_0^\\mathrm{T}\\frac{\\partial J_\\mathrm{e}}{\\partial \\mathbf{x}_0} + \\boldsymbol{\\varepsilon}\n\\]\n\n\nCode\nfig, axs = plt.subplots(ncols=2,constrained_layout=True)\ncmap=plt.get_cmap('tab10')\nfor i,key in enumerate(dJedx0_dict.keys()):\n    #fig, ax = plt.subplots()\n    Je_est = np.dot(X0.T,dJedx0_dict[key]) + err_dict[key]\n    if key=='diag':\n        axs[0].plot(Je,Je_est,lw=0.0,marker=markers[i],c=cmap(i+1),label=key,**marker_style)\n    else:\n        axs[1].plot(Je,Je_est,lw=0.0,marker=markers[i],c=cmap(i+1),label=key,**marker_style)\nfor ax in axs:\n    ymin, ymax = ax.get_ylim()\n    line = np.linspace(ymin,ymax,100)\n    ax.plot(line,line,color='k',zorder=0)\n    ax.set_xlabel('observed')\n    ax.set_ylabel('estimated')\n    ax.set_title('Je')\n    ax.legend()\n    #ax.set_title(key)\n    ax.grid()\n    ax.set_aspect(1.0)\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "EnASA_L96.html#ridge-regression",
    "href": "EnASA_L96.html#ridge-regression",
    "title": "Comparison of adjoint sensitivity analysis and ensemble adjoint sensitivity analysis using Lorenz-96 model",
    "section": "ridge regression",
    "text": "ridge regression\n\n\nCode\nnens = 40\nvt = 72\nmulist = [0.001,0.01,0.1,1.0]\ndJedx0_dict=dict()\nerr_dict=dict()\ndxeopt_dict=dict()\nxe = create_ens(nens)\nJe, Jes, Jem, sJe, X0, X0s, X0m, sX0 = generate_prtb(vt,xb,xe)\nfor mu in mulist:\n    dJedx0, err = enasa_ridge(Jes, Jem, sJe, X0s, X0m, sX0, mu=mu)\n    dJedx0_dict[mu] = dJedx0\n    err_dict[mu] = err\n    dxeopt = calc_dxopt(dJedx0,vt)\n    dxeopt_dict[mu] = dxeopt\nfig, ax = plt.subplots()\nax.plot(dJdx0_dict[vt],label='ASA')\nfor i,key in enumerate(dJedx0_dict.keys()):\n    ax.plot(dJedx0_dict[key],ls='dashed',marker=markers[i],label=r'$\\mu$='+f'{key}',**marker_style)\nax.legend()\nax.grid()\nax.set_title(f'dJ/dx0, vt={vt}h, Nens={nens}')\nplt.show()\n\nfig, ax = plt.subplots()\nax.plot(dxopt_dict[vt],label='ASA')\nfor i,key in enumerate(dxeopt_dict.keys()):\n    ax.plot(dxeopt_dict[key],ls='dashed',marker=markers[i],label=r'$\\mu$='+f'{key}',**marker_style)\nax.legend()\nax.grid()\nax.set_title(f'dxopt, vt={vt}h, Nens={nens}')\nplt.show()\n\nfig, ax = plt.subplots()\nfor i,key in enumerate(dJedx0_dict.keys()):\n    Je_est = np.dot(X0.T,dJedx0_dict[key]) + err_dict[key]\n    ax.plot(Je,Je_est,lw=0.0,marker=markers[i],c=cmap(i+1),label=r'$\\mu$='+f'{key}',**marker_style)\nymin, ymax = ax.get_ylim()\nline = np.linspace(ymin,ymax,100)\nax.plot(line,line,color='k',zorder=0)\nax.set_xlabel('observed')\nax.set_ylabel('estimated')\nax.set_title(f'Je, vt={vt}h, Nens={nens}')\nax.legend()\n#ax.set_title(key)\nax.grid()\nax.set_aspect(1.0)\nplt.show()"
  }
]